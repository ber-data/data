{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AJIMUOk6R-kU"
   },
   "outputs": [],
   "source": [
    "import click\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qk4rzj4yRCq5"
   },
   "outputs": [],
   "source": [
    "def get_all_packages(base_url, token):\n",
    "    \"\"\"Query the ESS-DIVE API to retrieve all data packages and their coordinates.\"\"\"\n",
    "    packages_endpoint = f\"{base_url}/packages\"\n",
    "    page_size = 100\n",
    "    row_start = 1\n",
    "    all_packages = []\n",
    "    total_packages = None\n",
    "    coordinates_table: List[Dict] = []\n",
    "\n",
    "    # Prompt for token if not provided\n",
    "    if not token:\n",
    "        token = click.prompt('Please enter your ESS-DIVE bearer token', hide_input=True)\n",
    "\n",
    "    headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Make GET request to the API with pagination parameters\n",
    "            params = {\n",
    "                'rowStart': row_start,\n",
    "                'pageSize': page_size,\n",
    "                'isPublic': 'true'\n",
    "            }\n",
    "            response = requests.get(packages_endpoint, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Parse JSON response\n",
    "            data = response.json()\n",
    "\n",
    "            # Break if no results\n",
    "            if not data.get('result'):\n",
    "                break\n",
    "\n",
    "            # Get total on first iteration\n",
    "            if total_packages is None:\n",
    "                total_packages = data.get('total', 0)\n",
    "\n",
    "            all_packages.extend(data['result'])\n",
    "            click.echo(f\"Retrieved {len(data['result'])} packages (total progress: {len(all_packages)}/{total_packages})\")\n",
    "\n",
    "            # Break if we've retrieved all packages\n",
    "            if len(all_packages) >= total_packages:\n",
    "                break\n",
    "\n",
    "            # Move to next page\n",
    "            row_start += page_size\n",
    "\n",
    "        # # Print summary table\n",
    "        # if coordinates_table:\n",
    "        #     click.echo(\"\\nSummary of Package Coordinates:\")\n",
    "        #     click.echo(tabulate(coordinates_table, headers='keys', tablefmt='grid'))\n",
    "        #     click.echo(f\"\\nTotal packages with coordinates: {len(coordinates_table)}\")\n",
    "        # else:\n",
    "        #     click.echo(\"\\nNo packages found with coordinates.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        click.echo(f\"Error querying ESS-DIVE API: {e}\", err=True)\n",
    "        raise click.Abort()\n",
    "    return all_packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K40BAvTVFwn"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HJYIX2TAVGVK"
   },
   "outputs": [],
   "source": [
    "url = \"https://api-sandbox.ess-dive.lbl.gov\"\n",
    "token=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nn_FGw87kyv2"
   },
   "outputs": [],
   "source": [
    "prod_url = \"https://api.ess-dive.lbl.gov\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BEVu98BNRXqL",
    "outputId": "81e8e25d-f84e-4177-d324-3f581943a515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 100 packages (total progress: 100/1200)\n",
      "Retrieved 100 packages (total progress: 200/1200)\n",
      "Retrieved 100 packages (total progress: 300/1200)\n",
      "Retrieved 100 packages (total progress: 400/1200)\n",
      "Retrieved 100 packages (total progress: 500/1200)\n",
      "Retrieved 100 packages (total progress: 600/1200)\n",
      "Retrieved 100 packages (total progress: 700/1200)\n",
      "Retrieved 100 packages (total progress: 800/1200)\n",
      "Retrieved 100 packages (total progress: 900/1200)\n",
      "Retrieved 100 packages (total progress: 1000/1200)\n",
      "Retrieved 100 packages (total progress: 1100/1200)\n",
      "Retrieved 100 packages (total progress: 1200/1200)\n"
     ]
    }
   ],
   "source": [
    "pkg_list=get_all_packages(prod_url, prod_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "z8J3EZP7UIZN"
   },
   "outputs": [],
   "source": [
    "# prompt: write a function to query ess-dive package API by id and return result\n",
    "\n",
    "def get_package_by_id(base_url, package_id, token):\n",
    "    \"\"\"Query the ESS-DIVE API for a specific package by ID.\"\"\"\n",
    "\n",
    "    packages_endpoint = f\"{base_url}/packages/{package_id}\"\n",
    "\n",
    "    headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(packages_endpoint, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "        data = response.json()\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying ESS-DIVE API: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JIHJNGINV_a4"
   },
   "outputs": [],
   "source": [
    "# prompt: given a bounding box with northwest and southeast coordinates, return centroid. input is in the form: [{'@type': 'GeoCoordinates', 'name': 'Northwest', 'latitude': 66.952, 'longitude': -168.14}, {'@type': 'GeoCoordinates', 'name': 'Southeast', 'latitude': 64.03, 'longitude': -159.19}]. Check for name Northwest and Southeast on input. If only one coordinate is provided, use that as centroid. Returned object should be a tuple\n",
    "\n",
    "def get_centroid(bounding_box):\n",
    "    \"\"\"\n",
    "    Calculates the centroid of a bounding box defined by northwest and southeast coordinates.\n",
    "\n",
    "    Args:\n",
    "        bounding_box: A list of dictionaries, where each dictionary represents a GeoCoordinate.\n",
    "                      It should contain items with 'name' as 'Northwest' and 'Southeast'.\n",
    "\n",
    "    Returns:\n",
    "        A tuple representing the centroid coordinates (latitude, longitude), or None if input is invalid.\n",
    "    \"\"\"\n",
    "    if not isinstance(bounding_box, list):\n",
    "        return None\n",
    "\n",
    "    if len(bounding_box) == 1:\n",
    "      # Use the single coordinate as the centroid\n",
    "      coordinate = bounding_box[0]\n",
    "      if coordinate.get('@type') == 'GeoCoordinates' and coordinate.get('latitude') is not None and coordinate.get('longitude') is not None:\n",
    "        return (coordinate.get('latitude'), coordinate.get('longitude'))\n",
    "      else:\n",
    "        return None\n",
    "\n",
    "    northwest = None\n",
    "    southeast = None\n",
    "    for coord in bounding_box:\n",
    "        if coord.get('name') == 'Northwest':\n",
    "            northwest = coord\n",
    "        elif coord.get('name') == 'Southeast':\n",
    "            southeast = coord\n",
    "\n",
    "    if northwest and southeast and northwest.get('@type') == 'GeoCoordinates' and southeast.get('@type') == 'GeoCoordinates':\n",
    "      try:\n",
    "        centroid_lat = (northwest['latitude'] + southeast['latitude']) / 2\n",
    "        centroid_lon = (northwest['longitude'] + southeast['longitude']) / 2\n",
    "        return (centroid_lat, centroid_lon)\n",
    "      except (KeyError, TypeError):\n",
    "        return None # Handle cases where latitude/longitude are missing or not numbers\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLCGBIlZJdrS",
    "outputId": "d50107ad-f2ae-4034-f557-d1065d874db4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ess-dive-19e50064aae312c-20250304T004733888',\n",
       " 'viewUrl': 'https://data.ess-dive.lbl.gov/view/doi:10.15485/2526687',\n",
       " 'url': 'https://api.ess-dive.lbl.gov/packages/ess-dive-19e50064aae312c-20250304T004733888',\n",
       " 'next': None,\n",
       " 'previous': 'https://api.ess-dive.lbl.gov/packages/ess-dive-c2f4ccc77b5ecb8-20250226T194451991',\n",
       " 'dateUploaded': '2025-03-04T00:47:34.917Z',\n",
       " 'dateModified': '2025-03-07T00:00:43.422Z',\n",
       " 'isPublic': True,\n",
       " 'citation': 'Carrero S; Fox P; Nico P (2025): Mineralogy of floodplain sediments from Meanders C, O, and Z in the East River Watershed, CO, USA. Watershed Function SFA. Dataset. doi:10.15485/2526687',\n",
       " 'dataset': {'@context': 'http://schema.org/',\n",
       "  '@type': 'Dataset',\n",
       "  '@id': 'doi:10.15485/2526687',\n",
       "  'name': 'Mineralogy of floodplain sediments from Meanders C, O, and Z in the East River Watershed, CO, USA',\n",
       "  'description': ['This dataset includes bulk X-ray diffraction data from floodplain sediments collected as a part of the Watershed Function Scientific Focus Area (SFA) located in the Upper Colorado River Basin. The data were collected in order to investigate the role of biogeochemical cycling and other river corridor processes on riverine export of solutes. Sediment cores were collected from Meander C, Meander O, and Meander Z in July 2016 to September 2017 to depths of approximately 40-95 cm. Sample metadata including locations, depths, and sample dates are included in a csv file (\"sample_list_and_locations.csv\"). The file \"diffraction_data.csv\" contains raw diffraction data, and mineral quantification is in the file \"mineral_abundance.csv\". This dataset also includes a file-level metadata (flmd.csv) file that lists each file contained in the dataset with associated metadata and a data dictionary (dd.csv) file that contains column/row headers used throughout the files along with a definition, units, and data type.'],\n",
       "  'alternateName': ['paf_374_374'],\n",
       "  'creator': [{'@type': 'Person',\n",
       "    '@id': 'https://orcid.org/0000-0003-3029-425X',\n",
       "    'givenName': 'Sergio',\n",
       "    'familyName': 'Carrero',\n",
       "    'affiliation': 'University of California, Berkeley',\n",
       "    'email': 'sergio.carrero@berkeley.edu'},\n",
       "   {'@type': 'Person',\n",
       "    '@id': 'https://orcid.org/0000-0002-5264-1876',\n",
       "    'givenName': 'Patricia',\n",
       "    'familyName': 'Fox',\n",
       "    'affiliation': 'Lawrence Berkeley National Laboratory',\n",
       "    'email': 'pmfox@lbl.gov'},\n",
       "   {'@type': 'Person',\n",
       "    '@id': 'https://orcid.org/0000-0002-4180-9397',\n",
       "    'givenName': 'Peter',\n",
       "    'familyName': 'Nico',\n",
       "    'affiliation': 'Lawrence Berkeley National Laboratory',\n",
       "    'email': 'psnico@lbl.gov'}],\n",
       "  'datePublished': '2025',\n",
       "  'keywords': ['XRD',\n",
       "   'minerals',\n",
       "   'floodplain',\n",
       "   'ESS-DIVE File Level Metadata Reporting Format',\n",
       "   'ESS-DIVE CSV File Formatting Guidelines Reporting Format',\n",
       "   'X-ray diffraction',\n",
       "   'soils'],\n",
       "  'variableMeasured': ['Mineralogy'],\n",
       "  'license': 'http://creativecommons.org/licenses/by/4.0/',\n",
       "  'spatialCoverage': [{'@type': 'Place',\n",
       "    'description': \"The East River (ER) is a snow‐dominated, headwater basin of the Upper Colorado River Basin (UCRB) located in the western United States. The ER is the designated testbed of Lawrence Berkeley National Laboratory's Watershed Function Scientific Focus Area (WFSFA). Through WFSFA, observational networks have been established to measure stream discharge and precipitation chemistry. The ER is considered representative of many snow‐dominated headwaters of the Rocky Mountains. The study domain encompasses nearly 85 square km, a 1.4‐km vertical drop in elevation (4,120 to 2,760 m) and pristine alpine, subalpine, montane, and riparian ecosystems. The ER contains high‐energy mountain streams to low‐energy meandering floodplains and is eroding primarily into the Cretaceous, carbon‐rich, marine shale of the Mancos Formation. Additional metadata on specific locations within the watershed are provided in the following related data package: Varadharajan C. et al. (2023) doi:10.15485/1660962\",\n",
       "    'geo': [{'@type': 'GeoCoordinates',\n",
       "      'name': 'Northwest',\n",
       "      'latitude': 39.034,\n",
       "      'longitude': -107.05},\n",
       "     {'@type': 'GeoCoordinates',\n",
       "      'name': 'Southeast',\n",
       "      'latitude': 38.88,\n",
       "      'longitude': -106.88}]}],\n",
       "  'award': ['DEAC0205CH11231 (Lawrence Berkeley National Laboratory)'],\n",
       "  'funder': [{'@type': 'Organization',\n",
       "    '@id': 'http://dx.doi.org/10.13039/100006206',\n",
       "    'name': 'U.S. DOE > Office of Science > Biological and Environmental Research (BER)'}],\n",
       "  'temporalCoverage': {'startDate': '2016-07-16',\n",
       "   'endDate': '2017-09-26',\n",
       "   '@type': 'DateTime'},\n",
       "  'editor': {'@type': 'Person',\n",
       "   '@id': 'https://orcid.org/0000-0002-5264-1876',\n",
       "   'givenName': 'Patricia',\n",
       "   'familyName': 'Fox',\n",
       "   'affiliation': 'Lawrence Berkeley National Laboratory',\n",
       "   'email': 'pmfox@lbl.gov'},\n",
       "  'citation': ['Additional metadata on specific locations within the watershed are provided in the following related data package:',\n",
       "   \"Varadharajan C ; Burrus M ; O'Ryan D ; Kakalia Z ; Alper E ; Banfield J ; Berkelhammer M ; Beutler C ; Brodie E ; Brown W ; Carbone M S ; Carroll R ; Christianson D ; Chou C ; Crystal-Ornelas R ; Chadwick K D ; Christensen J ; Dafflon B ; de Boer G ; Elbashandy H ; Enquist B J ; Feldman D ; Fox P ; Gilbert B ; Gochis D ; Henderson M ; Johnson D ; Kueppers L ; Li L ; Matheus Carnevali P ; Newman A ; Powell T ; Singha K ; Sorensen P ; Sprenger M ; Tokunaga T ; Versteeg R ; Wilkins M ; Williams K ; Worsham M ; Wong C ; Wu Y ; Zhang D ; Agarwal D (2023): Location Identifiers, Metadata, and Map for Field Measurements at the East River Watershed, Colorado, USA (Version 3.1). Watershed Function SFA, ESS-DIVE repository. Dataset. doi:10.15485/1660962\"],\n",
       "  'provider': {'@type': 'Organization',\n",
       "   'identifier': {'@type': 'PropertyValue',\n",
       "    'propertyID': 'ess-dive',\n",
       "    'value': '024b867d-25e3-4025-b68c-6598aded7aa2'},\n",
       "   'name': 'Watershed Function SFA',\n",
       "   'member': {'@type': 'Person',\n",
       "    'givenName': 'Eoin',\n",
       "    'familyName': 'Brodie',\n",
       "    'jobTitle': 'Principal Investigator',\n",
       "    'affiliation': 'Lawrence Berkeley National Laboratory',\n",
       "    'email': 'elbrodie@lbl.gov'}},\n",
       "  'measurementTechnique': ['Sediment samples were collected in approximately 15-cm increments using a 5 cm diameter soil core sampler equipped with a slide hammer until coarse alluvium was reached. After reaching coarse alluvium, sediment was sampled using an 8.3 cm diameter bucket auger. Sediment samples were placed in polyethylene bags, sealed into aluminized Mylar bags containing oxygen absorbers, shipped to the laboratory, and stored at 4°C until processing in order to preserve anaerobic conditions observed in some cores. Sediment samples were then sieved through a 2-mm sieve under field-moist conditions in a Coy anaerobic chamber filled with a gas mixture of 97% nitrogen and 3% hydrogen. The <2 mm fraction was then air-dried and ground to fine powder using a Tungsten-carbide ball mill prior to analysis.',\n",
       "   'Bulk X-ray diffraction (XRD) analyses were performed using a Rigaku SmartLab high-resolution XRD diffractometer with a Bragg-Brentano geometry. The diffractometer was equipped with a theta-theta goniometer and a rotating sample holder using Cu (λkα1 = 1.5406 Å and λkα2 = 1.5444 Å) cathode. Samples were placed in an aluminum holder to ensure random particle orientations and data were collected from 2 to 80 or 90 °2θ with a 0.02 °2θ step-size and count times of 2 s per step.'],\n",
       "  'distribution': [{'contentSize': 9.794921875,\n",
       "    'contentUrl': 'https://data.ess-dive.lbl.gov/catalog/d1/mn/v2/object/ess-dive-19e50064aae312c-20250304T004733888',\n",
       "    'encodingFormat': 'https://eml.ecoinformatics.org/eml-2.2.0',\n",
       "    'identifier': 'ess-dive-19e50064aae312c-20250304T004733888',\n",
       "    'name': 'Mineralogy_of_floodplain_sediments_from_Meanders.xml'},\n",
       "   {'contentSize': 0.576171875,\n",
       "    'contentUrl': 'https://data.ess-dive.lbl.gov/catalog/d1/mn/v2/object/ess-dive-25f8abbc84f45c7-20250304T004731693',\n",
       "    'encodingFormat': 'text/csv',\n",
       "    'identifier': 'ess-dive-25f8abbc84f45c7-20250304T004731693',\n",
       "    'name': 'flmd.csv'},\n",
       "   {'contentSize': 4.6650390625,\n",
       "    'contentUrl': 'https://data.ess-dive.lbl.gov/catalog/d1/mn/v2/object/ess-dive-a73e6e0bb262b3c-20250226T192334229',\n",
       "    'encodingFormat': 'text/csv',\n",
       "    'identifier': 'ess-dive-a73e6e0bb262b3c-20250226T192334229',\n",
       "    'name': 'dd.csv'},\n",
       "   {'contentSize': 1.96875,\n",
       "    'contentUrl': 'https://data.ess-dive.lbl.gov/catalog/d1/mn/v2/object/ess-dive-073f4a497854468-20250226T192334257',\n",
       "    'encodingFormat': 'text/csv',\n",
       "    'identifier': 'ess-dive-073f4a497854468-20250226T192334257',\n",
       "    'name': 'sample_list_and_locations.csv'},\n",
       "   {'contentSize': 617.30078125,\n",
       "    'contentUrl': 'https://data.ess-dive.lbl.gov/catalog/d1/mn/v2/object/ess-dive-3e06683df941037-20250226T192334240',\n",
       "    'encodingFormat': 'text/csv',\n",
       "    'identifier': 'ess-dive-3e06683df941037-20250226T192334240',\n",
       "    'name': 'diffraction_data.csv'},\n",
       "   {'contentSize': 2.7021484375,\n",
       "    'contentUrl': 'https://data.ess-dive.lbl.gov/catalog/d1/mn/v2/object/ess-dive-0b13e16b28c66b0-20250226T192334252',\n",
       "    'encodingFormat': 'text/csv',\n",
       "    'identifier': 'ess-dive-0b13e16b28c66b0-20250226T192334252',\n",
       "    'name': 'mineral_abundance.csv'}]}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_package_by_id(prod_url, 'ess-dive-19e50064aae312c-20250304T004733888', prod_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jW2Fj1hlsOkP",
    "outputId": "418052af-92a2-4bc0-8aac-fb67360e1467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ess-dive-a66bd8e860fd5d2-20250307T000457833\n",
      "Fetching ess-dive-b8764d466d15255-20250307T000152589\n",
      "Fetching ess-dive-3e3ca22cd60ddff-20250306T002220912\n",
      "Fetching ess-dive-4d4964c3c447a6b-20250304T233919265\n",
      "Fetching ess-dive-5492d7b0cb4dc5f-20250304T214840948\n",
      "Fetching ess-dive-123affc5bf658e5-20250304T212031404\n",
      "Fetching ess-dive-01e8ab563a78be1-20250304T211352752\n",
      "Fetching ess-dive-8375485eb34dd97-20250304T073447486\n",
      "Fetching ess-dive-19e50064aae312c-20250304T004733888\n",
      "Fetching ess-dive-939db401757e797-20250303T155212496\n",
      "Fetching ess-dive-5d67cc07bc39073-20250228T161607740\n",
      "Fetching ess-dive-f7b4f8a695ba8e3-20250227T225501084\n",
      "Fetching ess-dive-f265f7af37e0107-20250227T224411589\n",
      "Fetching ess-dive-39f4b4f961f5e3a-20250227T223242161\n",
      "Fetching ess-dive-aaa4e94dbaf22d1-20250227T184220226478\n",
      "Fetching ess-dive-b2d3f6f16e89545-20250226T143322810527\n",
      "Fetching ess-dive-0ca64974c5b9ef0-20250224T230909587\n",
      "Fetching ess-dive-e976198fe417dbb-20250224T215718432\n",
      "Fetching ess-dive-0c4579ec3e4bbad-20250224T215353274\n",
      "Fetching ess-dive-3531f1661cd538c-20250224T213650787\n"
     ]
    }
   ],
   "source": [
    "# prompt: for each package in pkg_list get package data and get centroids for all bounding boxes. create a dataframe with package_id, centroid latitude, centroid longitude\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming pkg_list, get_package_by_id, get_centroid, prod_url, and prod_token are defined as in the provided code.\n",
    "\n",
    "locations = []\n",
    "datasets = []\n",
    "for package in pkg_list[0:20]:  # Process the first 20 packages for demonstration\n",
    "    package_id = package['id']\n",
    "    print(f\"Fetching {package_id}\")\n",
    "\n",
    "    package_details = get_package_by_id(prod_url, package_id, prod_token)\n",
    "    if not package_details:\n",
    "        continue\n",
    "\n",
    "\n",
    "    dataset = package_details.get('dataset', {})\n",
    "    if dataset:\n",
    "        data_source = \"ESS-DIVE\"\n",
    "        url = package_details.get('viewUrl', '')\n",
    "        id = package_details.get('id', '')\n",
    "        alternate_ids = dataset.get('alternateName', '')\n",
    "        name = dataset.get('name', '')\n",
    "        description = '\\n'.join(dataset.get('description', ''))\n",
    "        date_uploaded = package_details.get('dateUploaded', '')\n",
    "        award = dataset.get('award', '')\n",
    "        citation = dataset.get('citation', '')\n",
    "        doi = dataset.get('@id', '')\n",
    "        spatial_coverage = dataset.get('spatialCoverage', [])\n",
    "        associated_researchers = []\n",
    "\n",
    "        pi = dataset['provider'].get('member', None)\n",
    "        associated_researchers.append({\n",
    "                'name': pi['givenName'] + ' ' + pi['familyName'],\n",
    "                'email': pi.get('email', ''),\n",
    "                'affiliation': pi.get('affiliation', ''),\n",
    "                'role': pi.get('jobTitle', '')\n",
    "            })\n",
    "\n",
    "\n",
    "        # TODO: Avoid duplication\n",
    "        editor = dataset.get('editor', None)\n",
    "        associated_researchers.append({\n",
    "                'name': editor['givenName'] + ' ' + editor['familyName'],\n",
    "                'email': editor.get('email', ''),\n",
    "                'affiliation': editor.get('affiliation', ''),\n",
    "                'role': 'Editor'\n",
    "            })\n",
    "\n",
    "        # TODO: Avoid duplication\n",
    "        for person in dataset.get('creator', None):\n",
    "              associated_researchers.append({\n",
    "                  'name': person['givenName'] + ' ' + person['familyName'],\n",
    "                  'email': person.get('email', ''),\n",
    "                  'affiliation': person.get('affiliation', ''),\n",
    "                  'role': 'Creator'\n",
    "              })\n",
    "\n",
    "\n",
    "\n",
    "        datasets.append({\n",
    "            'id': id,\n",
    "            'data_source': data_source,\n",
    "            'url': url,\n",
    "            'alternate_ids': alternate_ids,\n",
    "            'name': name,\n",
    "            'description': description,\n",
    "            'date_uploaded': date_uploaded,\n",
    "            'award': award,\n",
    "            'citation': citation,\n",
    "            'doi': doi,\n",
    "            'associated_researchers': associated_researchers,\n",
    "        })\n",
    "        for place in spatial_coverage:\n",
    "            centroid = get_centroid(place.get('geo', []))\n",
    "            if centroid:\n",
    "                locations.append({\n",
    "                    'dataset_id': id,\n",
    "                    'latitute': centroid[0],\n",
    "                    'longitude': centroid[1]\n",
    "                })\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Assuming datasets is defined as in the previous code\n",
    "\n",
    "# Convert the datasets list to a JSON string\n",
    "datasets_json = json.dumps(datasets, indent=4)\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open('datasets.json', 'w') as f:\n",
    "    f.write(datasets_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Assuming datasets is defined as in the previous code\n",
    "\n",
    "# Convert the datasets list to a JSON string\n",
    "geodata_json = json.dumps(locations, indent=4)\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open('geodata.json', 'w') as f:\n",
    "    f.write(geodata_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
